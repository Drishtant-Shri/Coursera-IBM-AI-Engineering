{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "OR_Rice_CLASSIFICATION_Model_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranesh6767/Coursera-IBM-AI-Engineering/blob/master/Classification_model/V2/OR_Rice_CLASSIFICATION_Model_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWFuhPWrZLhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORT REQUIRED LIBRARIES\n",
        "import os                                   #ACCESS DATA FROM FOLDERS AND REQUIRED PATHS\n",
        "import numpy as np                          #ARRAYS FOR DATA STORAGE AND WRANGLING\n",
        "import pandas as pd                         #DATAFRAME MANAGEMENT\n",
        "import torch                                #DEEP LEARNING FRAMEWORK PYTORCH\n",
        "import matplotlib.pyplot as plt             #PLOTTER LIBRARY\n",
        "import cv2                                  #COMPUTER VISION LIBRARY\n",
        "import torch.optim as optim                 #OPTIMIZER FROM PYTORCH\n",
        "import torch.nn as nn                       #NEURAL NETWORK CREATOR FROM TORCH\n",
        "import torch.nn.functional as F             #FUNCTION UTILIZATION FROM PYTORCH\n",
        "from random import shuffle                  #DATA SHUFFLING\n",
        "from torchvision import transforms          #TO TRANSFORM THE DATA/IMAGES AS PER OUR REQUIREMENT\n",
        "import PIL                                  #PYTHON IMAGING LIBRARY\n",
        "from PIL import Image                       #TO LOAD AND CLOSE IMAGE\n",
        "from shutil import copy                     #TO COPY DIRECTORIES/FILES\n",
        "from collections import OrderedDict         #CREATE ORDERED DICTIONARY\n",
        "from torchsummary import summary            #PRINTS SUMMARY OF THE SEQUENTIAL NETWORK\n",
        "from timeit import default_timer as timer   #IMPORT A TIMER TO TIME THE EPOCHS\n",
        "from datetime import datetime, timedelta    #DATE AND TIME OBJECT\n",
        "import shutil                               #TO COPY, DELETE, MOVE DATAFILES LIKE EXCEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpnfB5uEZcDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "59c6a453-c24c-4bb9-8620-2d1d925dc708"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiezNtYYZ2Py",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "006c2fe4-5f5a-4ae6-bbe3-813e268c5add"
      },
      "source": [
        "#shutil.copy('/content/drive/My Drive/RICE PROJECT/DATASET/ENCODED_ZIP/CUTOUTS_EN.zip','/content/')\n",
        "shutil.copy('/content/drive/My Drive/RICE PROJECT/DATASET/Augumented Dataset/ZIPS/AGClassification.zip','/content/')\n",
        "shutil.copy('/content/drive/My Drive/RICE PROJECT/DATASET/Augumented Dataset/ZIPS/AGriceBreeds.xlsx','/content/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/AGriceBreeds.xlsx'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbAHS827Z2K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_loc = '/content/AGClassification.zip'\n",
        "!mkdir 'Dataset'\n",
        "from shutil import unpack_archive\n",
        "unpack_archive(zip_loc, 'Dataset//')   #extract zip file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaTITnhgbLFU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37e7c155-6e22-4ff8-806c-c4dc1da558ba"
      },
      "source": [
        "len(os.listdir('/content/Dataset'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9vptW0zbK_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmNgo9GDZLhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def writeDataFrameExcel(filename, dataframe):\n",
        "    with pd.ExcelWriter(filename + '.xlsx') as writer:\n",
        "        dataframe.to_excel(writer, sheet_name = 'Sheet1')\n",
        "        print(\"Saved \",filename + '.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uEJjx86ZLhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRANSFORMS FOR DATA\n",
        "#Transform for Training data to the model\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(360),\n",
        "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                       #transforms.ColorJitter(brightness = (0.25, 0.75),contrast = (0.25, 0.75),saturation = (0.25, 0.75),hue = (-0.25, 0.25)),\n",
        "                                       #transforms.RandomCrop(size = (224, 224)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "#Transform for Test data to the model\n",
        "test_transforms = transforms.Compose([transforms.RandomRotation(360),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTNjI8FpNwBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
        "#torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
        "#torchvision.transforms.RandomVerticalFlip(p=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ4MfXGrZLhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#User Dependent Constants I\n",
        "numEpochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlQzOOLZLh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#User Dependent Constants II\n",
        "LearningRate  = 1.0e-4\n",
        "lrDiscount = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWmBDVQmZLh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#User Dependent Constants III\n",
        "minTrainSize = 0.925\n",
        "batchSize = 50\n",
        "epochStepPrint = 100\n",
        "printLossEveryEpoch = True\n",
        "saveModels = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WozHd8lhZLiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#MODEL 3\n",
        "\n",
        "#Define first block which can be used to build the classifier\n",
        "class netBlock1(nn.Module):\n",
        "    #Define data members of netBlock1\n",
        "    def __init__(self, iD, oD, kW = 3, kH = 3, ks = 1, p = 0):\n",
        "        super(netBlock1, self).__init__()\n",
        "        \n",
        "        self.block = nn.Sequential(nn.Conv2d(iD, iD,\n",
        "                                             (kW, kH), stride = ks,\n",
        "                                             padding = (int(kW/2) * p, int(kH/2) * p)),\n",
        "                                   nn.Conv2d(iD, oD,\n",
        "                                             (1, 1)))\n",
        "    \n",
        "    #Define a forward function to process the tensor in netBlock1\n",
        "    def forward(self, x):\n",
        "        return (self.block(x))\n",
        "    \n",
        "#Define second block which can be used to build the classifier\n",
        "class netBlock2(nn.Module):\n",
        "    #Define data members of netBlock2\n",
        "    def __init__(self, iD, oD, kW = 1, kH = 1, ks = 1, p = 0):\n",
        "        super(netBlock2, self).__init__()\n",
        "        \n",
        "        self.block = nn.Sequential(nn.Conv2d(iD, oD,\n",
        "                                             (kW, kH),\n",
        "                                             stride = ks))\n",
        "    \n",
        "    #Define a forward function to process the tensor in netBlock2\n",
        "    def forward(self, x):\n",
        "        return (self.block(x))\n",
        "\n",
        "#Define the classifier as a separate class\n",
        "class combinedBlock(nn.Module):\n",
        "    #Define data members of combinedBlock\n",
        "    def __init__(self, inputD, outputD, kernelW = 3, kernelH = 3, kernelS = 1, pad = 0):\n",
        "        super(combinedBlock, self).__init__()\n",
        "        \n",
        "        #Block 1 consists of 2 parallel processing blocks - A and B\n",
        "        #The two blocks are added to produce the final output of block 1\n",
        "        self.block_A = netBlock1(iD = inputD, oD = outputD,\n",
        "                                kW = kernelW, kH = kernelH, ks = kernelS, p = pad)\n",
        "        self.block_B = netBlock2(iD = inputD, oD = outputD,\n",
        "                                kW = 1, kH = 1, ks = 1, p = pad)\n",
        "        self.maxpool = nn.MaxPool2d((2, 2),stride = 2)\n",
        "        self.relu6 = nn.ReLU6()\n",
        "        self.bnorm = nn.BatchNorm2d(outputD)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.block_A(x) + self.block_B(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.bnorm(x)\n",
        "        return (x)\n",
        "\n",
        "class riceClassifier(nn.Module):\n",
        "    #Define data members of the classifier\n",
        "    def __init__(self):\n",
        "        super(riceClassifier, self).__init__()\n",
        "        \n",
        "        #Block 1 - 6\n",
        "        self.block1 = combinedBlock(inputD = 3, outputD = 16, pad = 1)\n",
        "        self.block2 = combinedBlock(inputD = 16, outputD = 32, pad = 1)\n",
        "        self.block3 = combinedBlock(inputD = 32, outputD = 64, pad = 1)\n",
        "        self.block4 = combinedBlock(inputD = 64, outputD = 128, pad = 1)\n",
        "        self.block5 = combinedBlock(inputD = 128, outputD = 256, pad = 1)\n",
        "        self.block6 = combinedBlock(inputD = 256, outputD = 512, pad = 1)\n",
        "        \n",
        "        #Average Pool\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size = 4, stride = 1)\n",
        "        #Flatten\n",
        "        self.flatten = nn.Flatten()\n",
        "        #Linear/Dense/Fully Connected\n",
        "        self.linear = nn.Linear(512, 18)\n",
        "        #LogSoftmax\n",
        "        self.logsoft = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.logsoft(x)\n",
        "    \n",
        "        return (x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTvQtp1-ZLiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adb25da1-f1f5-4998-cad3-bcb219169655"
      },
      "source": [
        "#Run this cell to redefine Model\n",
        "model = riceClassifier()\n",
        "model.float()\n",
        "model.cuda()           #FOR GPU SUPPORT\n",
        "\n",
        "#Print the summary of the model with input image dimensions\n",
        "summary(model, (3, 256, 256))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 3, 256, 256]              84\n",
            "            Conv2d-2         [-1, 16, 256, 256]              64\n",
            "         netBlock1-3         [-1, 16, 256, 256]               0\n",
            "            Conv2d-4         [-1, 16, 256, 256]              64\n",
            "         netBlock2-5         [-1, 16, 256, 256]               0\n",
            "         MaxPool2d-6         [-1, 16, 128, 128]               0\n",
            "             ReLU6-7         [-1, 16, 128, 128]               0\n",
            "       BatchNorm2d-8         [-1, 16, 128, 128]              32\n",
            "     combinedBlock-9         [-1, 16, 128, 128]               0\n",
            "           Conv2d-10         [-1, 16, 128, 128]           2,320\n",
            "           Conv2d-11         [-1, 32, 128, 128]             544\n",
            "        netBlock1-12         [-1, 32, 128, 128]               0\n",
            "           Conv2d-13         [-1, 32, 128, 128]             544\n",
            "        netBlock2-14         [-1, 32, 128, 128]               0\n",
            "        MaxPool2d-15           [-1, 32, 64, 64]               0\n",
            "            ReLU6-16           [-1, 32, 64, 64]               0\n",
            "      BatchNorm2d-17           [-1, 32, 64, 64]              64\n",
            "    combinedBlock-18           [-1, 32, 64, 64]               0\n",
            "           Conv2d-19           [-1, 32, 64, 64]           9,248\n",
            "           Conv2d-20           [-1, 64, 64, 64]           2,112\n",
            "        netBlock1-21           [-1, 64, 64, 64]               0\n",
            "           Conv2d-22           [-1, 64, 64, 64]           2,112\n",
            "        netBlock2-23           [-1, 64, 64, 64]               0\n",
            "        MaxPool2d-24           [-1, 64, 32, 32]               0\n",
            "            ReLU6-25           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
            "    combinedBlock-27           [-1, 64, 32, 32]               0\n",
            "           Conv2d-28           [-1, 64, 32, 32]          36,928\n",
            "           Conv2d-29          [-1, 128, 32, 32]           8,320\n",
            "        netBlock1-30          [-1, 128, 32, 32]               0\n",
            "           Conv2d-31          [-1, 128, 32, 32]           8,320\n",
            "        netBlock2-32          [-1, 128, 32, 32]               0\n",
            "        MaxPool2d-33          [-1, 128, 16, 16]               0\n",
            "            ReLU6-34          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
            "    combinedBlock-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,584\n",
            "           Conv2d-38          [-1, 256, 16, 16]          33,024\n",
            "        netBlock1-39          [-1, 256, 16, 16]               0\n",
            "           Conv2d-40          [-1, 256, 16, 16]          33,024\n",
            "        netBlock2-41          [-1, 256, 16, 16]               0\n",
            "        MaxPool2d-42            [-1, 256, 8, 8]               0\n",
            "            ReLU6-43            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-44            [-1, 256, 8, 8]             512\n",
            "    combinedBlock-45            [-1, 256, 8, 8]               0\n",
            "           Conv2d-46            [-1, 256, 8, 8]         590,080\n",
            "           Conv2d-47            [-1, 512, 8, 8]         131,584\n",
            "        netBlock1-48            [-1, 512, 8, 8]               0\n",
            "           Conv2d-49            [-1, 512, 8, 8]         131,584\n",
            "        netBlock2-50            [-1, 512, 8, 8]               0\n",
            "        MaxPool2d-51            [-1, 512, 4, 4]               0\n",
            "            ReLU6-52            [-1, 512, 4, 4]               0\n",
            "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
            "    combinedBlock-54            [-1, 512, 4, 4]               0\n",
            "        AvgPool2d-55            [-1, 512, 1, 1]               0\n",
            "          Flatten-56                  [-1, 512]               0\n",
            "           Linear-57                   [-1, 18]           9,234\n",
            "       LogSoftmax-58                   [-1, 18]               0\n",
            "================================================================\n",
            "Total params: 1,148,790\n",
            "Trainable params: 1,148,790\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 84.13\n",
            "Params size (MB): 4.38\n",
            "Estimated Total Size (MB): 89.27\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax10Bm-eZLif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8175211-6890-47a1-b95b-b47c7a6c7b09"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "riceClassifier(\n",
            "  (block1): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block2): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block3): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block4): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block5): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block6): combinedBlock(\n",
            "    (block_A): netBlock1(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (block_B): netBlock2(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (relu6): ReLU6()\n",
            "    (bnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
            "  (flatten): Flatten()\n",
            "  (linear): Linear(in_features=512, out_features=18, bias=True)\n",
            "  (logsoft): LogSoftmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqpofflFZLio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = LearningRate, amsgrad = True)\n",
        "lossCriterion = nn.NLLLoss(reduction = 'mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHCFMQ03ZLix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeTensorFromImageTrain(imgName):\n",
        "    #Open image using Pillow\n",
        "    image = Image.open(imgName)\n",
        "    #Use training transforms\n",
        "    imgTensor = train_transforms(image)\n",
        "    #Close the image\n",
        "    image.close()\n",
        "    #Return transformed image tensor\n",
        "    return(imgTensor)\n",
        "\n",
        "def makeTensorFromImageTest(imgName):\n",
        "    #Open Image using Pillow\n",
        "    image = Image.open(imgName)\n",
        "    #Use Testing Transforms\n",
        "    imgTensor = test_transforms(image)\n",
        "    #Close image\n",
        "    image.close()\n",
        "    #Return transformed image tensor\n",
        "    return(imgTensor)\n",
        "\n",
        "def getAccuracy(target, predicted):\n",
        "    #Get Predicted Class\n",
        "    classPredicted = predicted.argmax(1)\n",
        "    return((target==classPredicted).sum().item()/batchSize)\n",
        "\n",
        "def getConfusionMatrix_type(target1, predicted1, cmat1):\n",
        "    #Get the predicted class i.e. index which contains 1\n",
        "    classPredicted1 = predicted1.argmax(1)\n",
        "    #Number target tensors and predicted tensors form the columns and rows respectively\n",
        "    colsIter = target1.size()[0]\n",
        "    rowsIter = classPredicted1.size()[0]\n",
        "    #print(classPredicted1, target1)\n",
        "    target1 = target1.int()\n",
        "    \n",
        "    #Increment at index [predicted_class][target_class]\n",
        "    for i in range(min(target1.size()[0],classPredicted1.size()[0])):\n",
        "        cmat1[classPredicted1[i].item()][target1[i].item()] = cmat1[classPredicted1[i].item()][target1[i].item()] + 1\n",
        "\n",
        "\n",
        "def getF1Score_type(cmat1):\n",
        "    #Accumulate values along axis '1' i.e. columns\n",
        "    falsePos1 = cmat1.sum(1)\n",
        "    #Accumulate values along axis '0' i.e. rows\n",
        "    falseNeg1 = cmat1.sum(0)\n",
        "    #initialize an array for tru positive count\n",
        "    truePos1 = np.zeros(falsePos1.size)\n",
        "    \n",
        "    #Subtract true positives and negatives from false positives and negatives\n",
        "    for i in range(falsePos1.size):\n",
        "        falsePos1[i] = falsePos1[i] - cmat1[i][i]\n",
        "        falseNeg1[i] = falseNeg1[i] - cmat1[i][i]\n",
        "        truePos1[i] = cmat1[i][i]\n",
        "    \n",
        "    #Calculate the precision\n",
        "    precision1 = ((truePos1/(truePos1 + falsePos1 + 1)).sum())/falsePos1.size\n",
        "    #Calculate the recall\n",
        "    recall1 = ((truePos1/(truePos1 + falseNeg1 + 1)).sum())/falsePos1.size\n",
        "    \n",
        "    #Return Precision, Recall, F1 Score, Accuracy\n",
        "    return(precision1, recall1, ((2 * precision1 * recall1)/(precision1 + recall1)), truePos1.sum()/(cmat1.sum()))\n",
        "\n",
        "\n",
        "def trainModel(optimizer, lossCriterion, model, inputTensor, targetTensor):\n",
        "    #Initialize optimizer with zero gradient\n",
        "    optimizer.zero_grad()\n",
        "    inputTensor = inputTensor.cuda()\n",
        "    targetTensor = targetTensor.cuda().long()\n",
        "    #Pass the inputs through the model\n",
        "    output = model(inputTensor)\n",
        "    #Calculate the loss based on loss criterion used\n",
        "    loss = lossCriterion(output, targetTensor)\n",
        "    #Backpropagate through the loss\n",
        "    loss.backward()\n",
        "    #Step the optimizer\n",
        "    optimizer.step()\n",
        "    #Return the loss so that it can be added to the current loss\n",
        "    return(loss.item())\n",
        "\n",
        "#Used for Cross Validation\n",
        "def crossValidateModel(lossCriterion, model, inputTensor, targetTensor):\n",
        "    inputTensor = inputTensor.cuda()\n",
        "    targetTensor = targetTensor.cuda().long()\n",
        "    #Pass the inputs through the nodel\n",
        "    output = model(inputTensor)\n",
        "    #Find the loss but do not backpropagate\n",
        "    loss = lossCriterion(output, targetTensor)\n",
        "    #Return the loss so that it can be added to current loss\n",
        "    return(loss.item(), output)\n",
        "\n",
        "def lossFigure(LossArrayTrain, LossArrayVal, X, filename):\n",
        "    #Initialize figure\n",
        "    fig = plt.figure()\n",
        "    #Initialize aesthetic axes\n",
        "    ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
        "    #Add X and Y labels\n",
        "    ax.set_xlabel(\"Epoch Number\")\n",
        "    ax.set_ylabel(filename)\n",
        "    #Plot the Loss wrt the X Axis\n",
        "    ax.plot(X, LossArrayTrain, label = 'Training Loss')\n",
        "    ax.plot(X, LossArrayVal, label = 'Validation Loss')\n",
        "    #Place the legend on the upper left\n",
        "    ax.legend(loc = \"upper left\")\n",
        "    #Save the figure\n",
        "    fig.savefig(filename + '.png')\n",
        "    #Close the figure, delete allocated memory\n",
        "    plt.close(fig)\n",
        "\n",
        "def saveImage(valuesArray, X, filename):\n",
        "    #Initialize figure\n",
        "    fig = plt.figure()\n",
        "    #Initialize aesthetic axes\n",
        "    ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
        "    #Add X and Y labels\t\n",
        "    ax.set_xlabel(\"Epoch Number\")\n",
        "    ax.set_ylabel(filename)\n",
        "    #Plot the Values wrt the X Axis\n",
        "    ax.plot(X, valuesArray, label = filename)\n",
        "    #Place the legend on the upper left\n",
        "    ax.legend(loc = \"upper left\")\n",
        "    #Save the figure\n",
        "    fig.savefig(filename + '.png')\n",
        "    #Close the figure, delete allocated memory\n",
        "    plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgIAf_6LZLi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#path_folder = r\"E:/VIT/Second Year/SEM 2/EDI2/Main_Project_Folder/Rice/3. Excel Dataset Generation\"+\"/\"\n",
        "path_folder = \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySKrhkg9ZLjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_excel(str(path_folder)+'AGriceBreeds.xlsx',\n",
        "                     usecols = ['imageName', 'value', 'class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ijoTSGlX9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8fbcb363-7d85-4c5d-8c37-7ede8810a190"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageName</th>\n",
              "      <th>value</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/Dataset/R_steam113.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/Dataset/I_mis11.jpg</td>\n",
              "      <td>5</td>\n",
              "      <td>[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/Dataset/U_masuri116.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/Dataset/K_Shruti107.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/Dataset/K_boil130.jpg</td>\n",
              "      <td>15</td>\n",
              "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          imageName  ...                                  class\n",
              "0   /content/Dataset/R_steam113.jpg  ...  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
              "1      /content/Dataset/I_mis11.jpg  ...  [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
              "2  /content/Dataset/U_masuri116.jpg  ...  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
              "3  /content/Dataset/K_Shruti107.jpg  ...  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
              "4    /content/Dataset/K_boil130.jpg  ...  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRt6fRmPZLjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "029a642e-b006-4246-af91-c658506acc4d"
      },
      "source": [
        "randomVal = abs(np.random.randn())\n",
        "trainSize = randomVal - int(randomVal)\n",
        "while(trainSize < minTrainSize):\n",
        "    trainSize = trainSize + 0.05\n",
        "print(trainSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9316159700280772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNl1bYF2ZLjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Break the database into Train-CrossVal-Test\n",
        "#Create Training Dataset\n",
        "trainData = data.sample(frac = trainSize, replace = False, random_state = np.random.randint(len(data)) * np.random.randint(len(data))) #random state is a seed value\n",
        "\n",
        "#Create other data set to break into cross validation and test dataset\n",
        "otherData = data.drop(trainData.index)\n",
        "\n",
        "#Create Test and Cross Validation dataset\n",
        "crValData = otherData.sample(frac = 0.5, replace = False, random_state = np.random.randint(len(otherData)) * np.random.randint(len(otherData))) #random state is a seed value\n",
        "testData = otherData.drop(crValData.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOmVq6eeZLje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "60face08-a292-443d-9a45-4b875458a151"
      },
      "source": [
        "writeDataFrameExcel('trainData', trainData)\n",
        "writeDataFrameExcel('crValData', crValData)\n",
        "writeDataFrameExcel('testData', testData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved  trainData.xlsx\n",
            "Saved  crValData.xlsx\n",
            "Saved  testData.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdBdsVnTZLjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('TRAINING_DATA')\n",
        "save_path = 'TRAINING_DATA/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_M_-9J-ZLjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f68cb19f-c37a-4666-89cc-ee7c48238a1b"
      },
      "source": [
        "\n",
        "shutil.copy('testData.xlsx',save_path+'testData.xlsx')\n",
        "shutil.copy('trainData.xlsx',save_path+'trainData.xlsx')\n",
        "shutil.copy('crValData.xlsx',save_path+'crValData.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TRAINING_DATA/crValData.xlsx'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn5X3EKbZLj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50389350-0617-43e9-84e9-e4fc1e5615d6"
      },
      "source": [
        "#TO RELOAD THE TRAIN TEST AND CRVAL EXCEL SHEETS\n",
        "trainData = pd.read_excel(str(save_path) + 'trainData.xlsx',\n",
        "                          usecols = ['imageName', 'value', 'class'])\n",
        "crValData = pd.read_excel(str(save_path) +'crValData.xlsx',\n",
        "                          usecols = ['imageName', 'value', 'class'])\n",
        "testData = pd.read_excel(str(save_path) +'testData.xlsx',\n",
        "                         usecols = ['imageName', 'value', 'class'])\n",
        "trainratio = trainData.shape[0]/data.shape[0]\n",
        "#Print relevant sizes of training, validation and testing datasets\n",
        "print(\"Number of Images in: (1) Train = {tr:d}; (2) Val = {v:d}; (3) Test = {te:d}\"\n",
        "      .format(tr = len(trainData), v = len(crValData), te = len(testData)))\n",
        "\n",
        "print(\"Training Ratio: {z:f}\".format(z=trainratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Images in: (1) Train = 77697; (2) Val = 2852; (3) Test = 2851\n",
            "Training Ratio: 0.931619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UPYuOCRZLkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "44d8c649-3d35-4c8c-e80b-a74edb6f2bdd"
      },
      "source": [
        "#CREATE WEIGHTS FOR UNBALANCED SETS TO BE USED LATER (FOR TYPE ONLY)\n",
        "#Find unique type values\n",
        "uniqueType = list(set(list(data['value'])))\n",
        "\n",
        "#Initialize np arrays to store number of samples for every type\n",
        "typeNum = np.zeros(len(uniqueType))\n",
        "\n",
        "#Create subsets from unqiue types and count the samples\n",
        "for type in uniqueType:\n",
        "    typeNum[type] = len(data[data['value'] == type])\n",
        "\n",
        "#Scale number of types by total number of samples\n",
        "typeNum /= typeNum.sum()\n",
        "typeNum = typeNum.max()/typeNum\n",
        "\n",
        "#Print the weights which will be provided to type and grade loss criteria\n",
        "print(typeNum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.56571429 1.76774194 1.67073171 1.096      1.58381503 2.20967742\n",
            " 1.60233918 1.52222222 1.53932584 1.23423423 1.48108108 1.6809816\n",
            " 2.10769231 1.57471264 1.5480226  1.         1.16595745 1.33009709]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeBSZyDjZLkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63ee3ef7-5a7c-4030-8d74-78bc17e0049d"
      },
      "source": [
        "#Image Names of All three data sets\n",
        "trainImageNames = list(trainData['imageName'])\n",
        "crvalImageNames = list(crValData['imageName'])\n",
        "testImageNames = list(testData['imageName'])\n",
        "#Value of All three data sets\n",
        "trainvalue = list(trainData['value'])\n",
        "crvalvalue = list(crValData['value'])\n",
        "testvalue = list(testData['value'])\n",
        "#Class of All 3 data sets\n",
        "trainclass = list(trainData['class'])\n",
        "crvalclass = list(crValData['class'])\n",
        "testclass = list(testData['class'])\n",
        "\n",
        "print(\"Number of Images: (1) Training : \", len(trainImageNames), \", (2) Cross Val : \", len(crvalImageNames), \", (3) Test : \", len(testImageNames))\n",
        "\n",
        "#Loop Sizes and Remainder images based on mini batch size\n",
        "trainRems = len(trainImageNames) % batchSize\n",
        "crvalRems = len(crvalImageNames) % batchSize\n",
        "testRems = len(testImageNames) % batchSize\n",
        "trainLoops = int((len(trainImageNames) - trainRems)/batchSize)\n",
        "crvalLoops = int((len(crvalImageNames) - crvalRems)/batchSize)\n",
        "testLoops = int((len(testImageNames) - testRems)/batchSize)\n",
        "\n",
        "#Print relevant values: sizes and loops of train, validat ion and test datasets\n",
        "print(\"Number of Images in: (1) Train = {tr:d}; (2) Val = {v:d}; (3) Test = {te:d}\"\n",
        "      .format(tr = len(trainImageNames), v = len(crvalImageNames), te = len(testImageNames)))\n",
        "print(\"Number of Loops in : (1) Train = {tr:d}; (2) Val = {v:d}; (3) Test = {te:d}\"\n",
        "      .format(tr = trainLoops, v = crvalLoops, te = testLoops))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Images: (1) Training :  77697 , (2) Cross Val :  2852 , (3) Test :  2851\n",
            "Number of Images in: (1) Train = 77697; (2) Val = 2852; (3) Test = 2851\n",
            "Number of Loops in : (1) Train = 1553; (2) Val = 57; (3) Test = 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPFFLlGhZLkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlG8DsUPZLkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Epoch'] = None\n",
        "df['F1_Type'] = None\n",
        "df['Recall_Type'] = None\n",
        "df['Precision_Type'] = None\n",
        "df['CrossvalAccuracy_Type'] = None\n",
        "df['Train Loss_Type'] = None\n",
        "df['CrVal Loss_Type'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn4bhKElZLks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#USE ONLY IF THE MODEL IS TO BE RETRAINED\n",
        "#retrainModel = os.path.join('', '')\n",
        "#model = torch.load(os.path.join(os.path.join(os.getcwd(), 'networks'), retrainModel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjn37HcHZLk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2e4ceb69-390f-43e1-a1f0-71f5c6e4c9d7"
      },
      "source": [
        "#Initialize Optimizer and Loss : Adam and Negative Log Likelihood Loss\n",
        "#Reinitialize the optimzer everytime you change the learning rate\n",
        "#REINITIALIZATION IS IMPORTANT WHEN RETRAINING SAVED MODELS TO OPTIMIZE FOR PREDICTION METRICS\n",
        "\n",
        "#Define Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = LearningRate, amsgrad = True)\n",
        "\n",
        "#Initialize the Loss for wheat types using sample percentage for each type\n",
        "lossCriterion = nn.NLLLoss(reduction = 'mean', weight = torch.FloatTensor(typeNum).cuda())\n",
        "\n",
        "print(optimizer)\n",
        "print(lossCriterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0\n",
            ")\n",
            "NLLLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REIU1wnAZLlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('/content/TRAINING_DATA/MODELS')\n",
        "model_dir_path = r\"/content/TRAINING_DATA/MODELS/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U18riUlmZLlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17477e61-a626-49b0-b59d-eeb5c46af3c2"
      },
      "source": [
        "#Numpy arrays for storing:\n",
        "# Training Loss\n",
        "trainLossType = np.zeros(numEpochs)\n",
        "#Cross Validation Loss\n",
        "crvalLossType = np.zeros(numEpochs)\n",
        "#Accuracy\n",
        "accuracyType = np.zeros(numEpochs)\n",
        "#F1 Score\n",
        "f1ScoreType = np.zeros(numEpochs)\n",
        "\n",
        "'''\n",
        "True positive: diagonal position, cm(x, x).\n",
        "False positive: sum of column x (without main diagonal), sum(cm(:, x))-cm(x, x).\n",
        "False negative: sum of row x (without main diagonal), sum(cm(x, :), 2)-cm(x, x).\n",
        "'''\n",
        "\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    #Print the Epoch Number and the Learning rate which will be used in this epoch\n",
        "    print(\"Epoch Number = {e:3d}, Learning Rate = {lr:8.7f}\"\n",
        "          .format(e = epoch + 1, lr = LearningRate))\n",
        "    #Numpy array as confusion matrix to calculate Precision, Recall and F1 Score\n",
        "    #The confusion matrix is reinitialized every epoch to find the metrics in that specific epoch\n",
        "    #Confusion Matrix for wheat type\n",
        "    confusionMatrixType = np.zeros((18, 18))\n",
        "    \n",
        "    #Shuffle the Training and Cross Validation Datasets for the epoch\n",
        "    trainData = trainData.sample(frac = 1,\n",
        "                                 replace = False,\n",
        "                                 random_state = np.random.randint(len(trainData)/2) * \n",
        "                                                 np.random.randint(len(trainData)/2)) #random state is a seed value\n",
        "    crValData = crValData.sample(frac = 1,\n",
        "                                 replace = False,\n",
        "                                 random_state = np.random.randint(len(crValData)/2) * \n",
        "                                                 np.random.randint(len(crValData)/2)) #random state is a seed value\n",
        "    \n",
        "    #Recollect the imageNames of Shuffled Train and Validation Datasets\n",
        "    trainImageNames = list(trainData['imageName'])\n",
        "    crvalImageNames = list(crValData['imageName'])\n",
        "    \n",
        "    #Target 1 of All three data sets\n",
        "    traintarget1 = list(trainData['value'])\n",
        "    crvaltarget1 = list(crValData['value'])\n",
        "\n",
        "    #Initialize the optimizer for the epoch\n",
        "    optimizer = optim.Adam(model.parameters(), lr = LearningRate, amsgrad = True)\n",
        "    #Discount the learning rate\n",
        "    LearningRate *= lrDiscount\n",
        "    \n",
        "    #Start Timer for epoch-duration measurement\n",
        "    start = timer()\n",
        "    \n",
        "    #Break training data into mini batches\n",
        "    for i in range(trainLoops + 1):\n",
        "        \n",
        "        #Get Start index for batch\n",
        "        startNum = i * batchSize\n",
        "        \n",
        "        #Get Ending index for batch\n",
        "        if((startNum + batchSize - 1) < (len(trainImageNames) - 1)):\n",
        "            endNum = startNum + batchSize - 1\n",
        "        else:\n",
        "            endNum = startNum + trainRems - 1\n",
        "        \n",
        "        #Initialize the lists for training to store: \n",
        "        #input Tensors\n",
        "        #values of wheat type and \n",
        "        #values of wheat grade\n",
        "        tensorList = list()\n",
        "        target1List = list()\n",
        "        \n",
        "        \n",
        "        #Get List of Tensors from Images in dataset\n",
        "        for j in range(startNum, endNum + 1):\n",
        "            #Append transformed tensor of training images of user defined batch size\n",
        "            tensorList.append(makeTensorFromImageTrain(trainImageNames[j]))\n",
        "            \n",
        "            #Append class of training output tensor to the class list\n",
        "            target1List.append(torch.FloatTensor([traintarget1[j]]))\n",
        "            \n",
        "        #Check if tensor list contains only one element\n",
        "        if(len(tensorList) < 2): break\n",
        "        \n",
        "        #Create Training Mini Batch Tensors - input, wheat type output, wheat grade output\n",
        "        miniBatchInputTensor = torch.stack(tensorList)\n",
        "        \n",
        "        \n",
        "        miniBatchOutput1Tensor = torch.stack(target1List).squeeze()        \n",
        "        \n",
        "        #TRAIN THE MODELS AND GET TRAINING LOSSES\n",
        "        lossType = trainModel(optimizer,\n",
        "                                         lossCriterion,\n",
        "                                         model,\n",
        "                                         miniBatchInputTensor,\n",
        "                                         miniBatchOutput1Tensor)\n",
        "        #Accumulate Training Loss for Rice Type Classification\n",
        "        trainLossType[epoch] += lossType/(trainLoops + 1)\n",
        "        \n",
        "        #Print the losses every few predefined number of epochs\n",
        "        if (((i + 1) % epochStepPrint) == 0) : \n",
        "            print(\"\\tTraining Loop #{a:5d}, Loss = {tl:6.4f}\"\n",
        "                  .format(a = (i + 1), tl = lossType))\n",
        "\n",
        "        #TRAINING ENDS HERE\n",
        "  \n",
        "    #Break Cross Val data into mini batches\t\n",
        "    for i in range(crvalLoops + 1):\n",
        "        \n",
        "        #Get Start index for batch\n",
        "        startNum = i * batchSize\n",
        "        \n",
        "        #Get Ending index for batch\n",
        "        if((startNum + batchSize - 1) < (len(crvalImageNames) - 1)):\n",
        "            endNum = startNum + batchSize - 1\n",
        "        else:\n",
        "            endNum = startNum + crvalRems - 1\n",
        "        \n",
        "        #Initialize the lists for cross validation to store: \n",
        "        #input Tensors\n",
        "        #values of wheat type and \n",
        "        #values of wheat grade\n",
        "        tensorList = list()\n",
        "        target1List = list()\n",
        "        \n",
        "        \n",
        "        #Get List of Tensors from Images in dataset\n",
        "        for j in range(startNum, endNum + 1):\n",
        "            #Append transformed tensor of CrVal images of user defined batch size\n",
        "            tensorList.append(makeTensorFromImageTrain(crvalImageNames[j]))\n",
        "            \n",
        "            #Append class of training output tensor to the class list\n",
        "            target1List.append(torch.FloatTensor([crvaltarget1[j]]))\n",
        "            \n",
        "        \n",
        "        #Check if tensor list contains only one element\n",
        "        if(len(tensorList) < 2): break\n",
        "        \n",
        "        \n",
        "        miniBatchInputTensor = torch.stack(tensorList)       \n",
        "        miniBatchOutput1Tensor = torch.stack(target1List).squeeze()        \n",
        "       \n",
        "        #GET CROSS VALIDATION LOSS\n",
        "        lossType, outputTensor1, = crossValidateModel(\n",
        "                                         lossCriterion,\n",
        "                                         model,\n",
        "                                         miniBatchInputTensor,\n",
        "                                         miniBatchOutput1Tensor)\n",
        "        #Accumulate cross validation loss for rice type classification\n",
        "        crvalLossType[epoch] += lossType/(crvalLoops + 1)\n",
        "        \n",
        "        #Update the epoch's confusion matrix for rice type classification\n",
        "        getConfusionMatrix_type(miniBatchOutput1Tensor, outputTensor1, confusionMatrixType)\n",
        "        \n",
        "        if (((i + 1) % (epochStepPrint/10)) == 0) : \n",
        "            print(\"\\tValidation Loop #{a:4d}, Loss = {tl:6.4f}\"\n",
        "                  .format(a = (i + 1), tl = lossType))\n",
        "        \n",
        "        #CROSS VALIDATION ENDS HERE\n",
        "    \n",
        "    #End Timer for Epoch-duration measurement\n",
        "    end = timer()\n",
        "\n",
        "    #Print loss every epoch if the flag to print every epoch is true\n",
        "    if(printLossEveryEpoch):\n",
        "        \n",
        "        #Print the epoch number\n",
        "        print(\"Epoch Number = {n:2d}, Time Taken (Mins) = {t:6.4f}\"\n",
        "              .format(n = (epoch + 1), t = (end - start)/60))\n",
        "        \n",
        "        #Find the values of above headers\n",
        "        precision1, recall1, f1ScoreType[epoch], accuracyType[epoch] = getF1Score_type(confusionMatrixType)\n",
        "                \n",
        "        torch.save(model,model_dir_path + str(epoch+1) +'_'+ str(f1ScoreType[epoch]) + '.pth')\n",
        "        modelName = \"riceBreedModelDICTIONARY_\" +str(epoch+1) +'_' +str(f1ScoreType[epoch]) + \".pth\"\n",
        "        torch.save(model.state_dict(), model_dir_path + str(modelName))\n",
        "        print(\"Model and Model Dictionary Saved\")\n",
        "        \n",
        "\n",
        "        df = df.append({'Epoch':epoch+1,'F1_Type':f1ScoreType[epoch],'Precision_Type': precision1,'Recall_Type': recall1, 'CrossvalAccuracy_Type': accuracyType[epoch], 'Train Loss_Type': trainLossType[epoch],'CrVal Loss_Type': crvalLossType[epoch] },ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #Print required metric values\n",
        "        #Print training loss of wheat type and grade classification\n",
        "        print(\"Train Loss  = ({l1:6.4f})\"\n",
        "              .format(l1 = trainLossType[epoch]))\n",
        "        \n",
        "        #Print Cross Val loss of wheat type and grade classification\n",
        "        print(\"CrVal Loss  = ({l1:6.4f})\"\n",
        "              .format(l1 = crvalLossType[epoch]))\n",
        "        \n",
        "        #Print Cross Val Accuracy of wheat type and grade classification\n",
        "        print(\"Cr-Val Acc  = ({l1:6.4f})\"\n",
        "              .format(l1 = accuracyType[epoch]))\n",
        "        \n",
        "        #Print F1 Score of wheat type and grade classification\n",
        "        print(\"F1 Score  = ({l1:6.4f})\"\n",
        "              .format(l1 = f1ScoreType[epoch]))\n",
        "        \n",
        "        #Print Precision of wheat type and grade classification\n",
        "        print(\"Precision  = ({l1:6.4f})\"\n",
        "              .format(l1 = precision1))\n",
        "        \n",
        "        #Print Recall of wheat type and grade classification\n",
        "        print(\"Recall  = ({l1:6.4f})\"\n",
        "              .format(l1 = recall1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Number =   1, Learning Rate = 0.0001000\n",
            "\tTraining Loop #  100, Loss = 1.0416\n",
            "\tTraining Loop #  200, Loss = 1.0321\n",
            "\tTraining Loop #  300, Loss = 1.0334\n",
            "\tTraining Loop #  400, Loss = 0.7413\n",
            "\tTraining Loop #  500, Loss = 0.5694\n",
            "\tTraining Loop #  600, Loss = 0.5768\n",
            "\tTraining Loop #  700, Loss = 0.5641\n",
            "\tTraining Loop #  800, Loss = 0.7302\n",
            "\tTraining Loop #  900, Loss = 0.7420\n",
            "\tTraining Loop # 1000, Loss = 0.5819\n",
            "\tTraining Loop # 1100, Loss = 0.7295\n",
            "\tTraining Loop # 1200, Loss = 0.5615\n",
            "\tTraining Loop # 1300, Loss = 0.7218\n",
            "\tTraining Loop # 1400, Loss = 0.7912\n",
            "\tTraining Loop # 1500, Loss = 0.4121\n",
            "\tValidation Loop #  10, Loss = 0.6572\n",
            "\tValidation Loop #  20, Loss = 0.4944\n",
            "\tValidation Loop #  30, Loss = 0.4935\n",
            "\tValidation Loop #  40, Loss = 0.6102\n",
            "\tValidation Loop #  50, Loss = 0.6186\n",
            "Epoch Number =  1, Time Taken (Mins) = 10.3080\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.7527)\n",
            "CrVal Loss  = (0.5898)\n",
            "Cr-Val Acc  = (0.8096)\n",
            "F1 Score  = (0.8076)\n",
            "Precision  = (0.8124)\n",
            "Recall  = (0.8030)\n",
            "Epoch Number =   2, Learning Rate = 0.0000950\n",
            "\tTraining Loop #  100, Loss = 0.5960\n",
            "\tTraining Loop #  200, Loss = 0.4183\n",
            "\tTraining Loop #  300, Loss = 0.4608\n",
            "\tTraining Loop #  400, Loss = 0.3986\n",
            "\tTraining Loop #  500, Loss = 0.5014\n",
            "\tTraining Loop #  600, Loss = 0.6696\n",
            "\tTraining Loop #  700, Loss = 0.4348\n",
            "\tTraining Loop #  800, Loss = 0.4980\n",
            "\tTraining Loop #  900, Loss = 0.5074\n",
            "\tTraining Loop # 1000, Loss = 0.3039\n",
            "\tTraining Loop # 1100, Loss = 0.4607\n",
            "\tTraining Loop # 1200, Loss = 0.1996\n",
            "\tTraining Loop # 1300, Loss = 0.4107\n",
            "\tTraining Loop # 1400, Loss = 0.2702\n",
            "\tTraining Loop # 1500, Loss = 0.6125\n",
            "\tValidation Loop #  10, Loss = 0.3861\n",
            "\tValidation Loop #  20, Loss = 0.4922\n",
            "\tValidation Loop #  30, Loss = 0.5300\n",
            "\tValidation Loop #  40, Loss = 0.3757\n",
            "\tValidation Loop #  50, Loss = 0.2621\n",
            "Epoch Number =  2, Time Taken (Mins) = 10.3910\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.4334)\n",
            "CrVal Loss  = (0.3974)\n",
            "Cr-Val Acc  = (0.8650)\n",
            "F1 Score  = (0.8597)\n",
            "Precision  = (0.8614)\n",
            "Recall  = (0.8580)\n",
            "Epoch Number =   3, Learning Rate = 0.0000902\n",
            "\tTraining Loop #  100, Loss = 0.4679\n",
            "\tTraining Loop #  200, Loss = 0.3066\n",
            "\tTraining Loop #  300, Loss = 0.2489\n",
            "\tTraining Loop #  400, Loss = 0.3888\n",
            "\tTraining Loop #  500, Loss = 0.3174\n",
            "\tTraining Loop #  600, Loss = 0.3354\n",
            "\tTraining Loop #  700, Loss = 0.3704\n",
            "\tTraining Loop #  800, Loss = 0.5463\n",
            "\tTraining Loop #  900, Loss = 0.2317\n",
            "\tTraining Loop # 1000, Loss = 0.3658\n",
            "\tTraining Loop # 1100, Loss = 0.3028\n",
            "\tTraining Loop # 1200, Loss = 0.4638\n",
            "\tTraining Loop # 1300, Loss = 0.2448\n",
            "\tTraining Loop # 1400, Loss = 0.3372\n",
            "\tTraining Loop # 1500, Loss = 0.4258\n",
            "\tValidation Loop #  10, Loss = 0.1715\n",
            "\tValidation Loop #  20, Loss = 0.3473\n",
            "\tValidation Loop #  30, Loss = 0.2465\n",
            "\tValidation Loop #  40, Loss = 0.3784\n",
            "\tValidation Loop #  50, Loss = 0.3697\n",
            "Epoch Number =  3, Time Taken (Mins) = 10.1383\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.3377)\n",
            "CrVal Loss  = (0.3014)\n",
            "Cr-Val Acc  = (0.8969)\n",
            "F1 Score  = (0.8923)\n",
            "Precision  = (0.8928)\n",
            "Recall  = (0.8917)\n",
            "Epoch Number =   4, Learning Rate = 0.0000857\n",
            "\tTraining Loop #  100, Loss = 0.3206\n",
            "\tTraining Loop #  200, Loss = 0.1697\n",
            "\tTraining Loop #  300, Loss = 0.3367\n",
            "\tTraining Loop #  400, Loss = 0.1898\n",
            "\tTraining Loop #  500, Loss = 0.4140\n",
            "\tTraining Loop #  600, Loss = 0.2447\n",
            "\tTraining Loop #  700, Loss = 0.1530\n",
            "\tTraining Loop #  800, Loss = 0.3194\n",
            "\tTraining Loop #  900, Loss = 0.2482\n",
            "\tTraining Loop # 1000, Loss = 0.2792\n",
            "\tTraining Loop # 1100, Loss = 0.4015\n",
            "\tTraining Loop # 1200, Loss = 0.3394\n",
            "\tTraining Loop # 1300, Loss = 0.2964\n",
            "\tTraining Loop # 1400, Loss = 0.2889\n",
            "\tTraining Loop # 1500, Loss = 0.2654\n",
            "\tValidation Loop #  10, Loss = 0.2518\n",
            "\tValidation Loop #  20, Loss = 0.2125\n",
            "\tValidation Loop #  30, Loss = 0.2425\n",
            "\tValidation Loop #  40, Loss = 0.3720\n",
            "\tValidation Loop #  50, Loss = 0.1847\n",
            "Epoch Number =  4, Time Taken (Mins) = 10.1191\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.2818)\n",
            "CrVal Loss  = (0.2376)\n",
            "Cr-Val Acc  = (0.9158)\n",
            "F1 Score  = (0.9079)\n",
            "Precision  = (0.9058)\n",
            "Recall  = (0.9099)\n",
            "Epoch Number =   5, Learning Rate = 0.0000815\n",
            "\tTraining Loop #  100, Loss = 0.1790\n",
            "\tTraining Loop #  200, Loss = 0.4176\n",
            "\tTraining Loop #  300, Loss = 0.3160\n",
            "\tTraining Loop #  400, Loss = 0.1988\n",
            "\tTraining Loop #  500, Loss = 0.2412\n",
            "\tTraining Loop #  600, Loss = 0.1194\n",
            "\tTraining Loop #  700, Loss = 0.2462\n",
            "\tTraining Loop #  800, Loss = 0.2453\n",
            "\tTraining Loop #  900, Loss = 0.2097\n",
            "\tTraining Loop # 1000, Loss = 0.2082\n",
            "\tTraining Loop # 1100, Loss = 0.3713\n",
            "\tTraining Loop # 1200, Loss = 0.3565\n",
            "\tTraining Loop # 1300, Loss = 0.1054\n",
            "\tTraining Loop # 1400, Loss = 0.4088\n",
            "\tTraining Loop # 1500, Loss = 0.2246\n",
            "\tValidation Loop #  10, Loss = 0.1907\n",
            "\tValidation Loop #  20, Loss = 0.1277\n",
            "\tValidation Loop #  30, Loss = 0.1435\n",
            "\tValidation Loop #  40, Loss = 0.3074\n",
            "\tValidation Loop #  50, Loss = 0.3342\n",
            "Epoch Number =  5, Time Taken (Mins) = 10.3525\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.2410)\n",
            "CrVal Loss  = (0.2495)\n",
            "Cr-Val Acc  = (0.9239)\n",
            "F1 Score  = (0.9164)\n",
            "Precision  = (0.9174)\n",
            "Recall  = (0.9155)\n",
            "Epoch Number =   6, Learning Rate = 0.0000774\n",
            "\tTraining Loop #  100, Loss = 0.2521\n",
            "\tTraining Loop #  200, Loss = 0.1064\n",
            "\tTraining Loop #  300, Loss = 0.1789\n",
            "\tTraining Loop #  400, Loss = 0.0904\n",
            "\tTraining Loop #  500, Loss = 0.3112\n",
            "\tTraining Loop #  600, Loss = 0.1343\n",
            "\tTraining Loop #  700, Loss = 0.3109\n",
            "\tTraining Loop #  800, Loss = 0.1785\n",
            "\tTraining Loop #  900, Loss = 0.1571\n",
            "\tTraining Loop # 1000, Loss = 0.1415\n",
            "\tTraining Loop # 1100, Loss = 0.2245\n",
            "\tTraining Loop # 1200, Loss = 0.1947\n",
            "\tTraining Loop # 1300, Loss = 0.2097\n",
            "\tTraining Loop # 1400, Loss = 0.4390\n",
            "\tTraining Loop # 1500, Loss = 0.1688\n",
            "\tValidation Loop #  10, Loss = 0.2021\n",
            "\tValidation Loop #  20, Loss = 0.1530\n",
            "\tValidation Loop #  30, Loss = 0.1685\n",
            "\tValidation Loop #  40, Loss = 0.1568\n",
            "\tValidation Loop #  50, Loss = 0.2594\n",
            "Epoch Number =  6, Time Taken (Mins) = 10.3587\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.2107)\n",
            "CrVal Loss  = (0.1995)\n",
            "Cr-Val Acc  = (0.9292)\n",
            "F1 Score  = (0.9229)\n",
            "Precision  = (0.9240)\n",
            "Recall  = (0.9218)\n",
            "Epoch Number =   7, Learning Rate = 0.0000735\n",
            "\tTraining Loop #  100, Loss = 0.2006\n",
            "\tTraining Loop #  200, Loss = 0.1861\n",
            "\tTraining Loop #  300, Loss = 0.2043\n",
            "\tTraining Loop #  400, Loss = 0.0743\n",
            "\tTraining Loop #  500, Loss = 0.2099\n",
            "\tTraining Loop #  600, Loss = 0.1358\n",
            "\tTraining Loop #  700, Loss = 0.1851\n",
            "\tTraining Loop #  800, Loss = 0.3201\n",
            "\tTraining Loop #  900, Loss = 0.2714\n",
            "\tTraining Loop # 1000, Loss = 0.1554\n",
            "\tTraining Loop # 1100, Loss = 0.2498\n",
            "\tTraining Loop # 1200, Loss = 0.1498\n",
            "\tTraining Loop # 1300, Loss = 0.2188\n",
            "\tTraining Loop # 1400, Loss = 0.1023\n",
            "\tTraining Loop # 1500, Loss = 0.1430\n",
            "\tValidation Loop #  10, Loss = 0.2601\n",
            "\tValidation Loop #  20, Loss = 0.2472\n",
            "\tValidation Loop #  30, Loss = 0.2265\n",
            "\tValidation Loop #  40, Loss = 0.1571\n",
            "\tValidation Loop #  50, Loss = 0.1641\n",
            "Epoch Number =  7, Time Taken (Mins) = 10.1413\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.1893)\n",
            "CrVal Loss  = (0.1919)\n",
            "Cr-Val Acc  = (0.9407)\n",
            "F1 Score  = (0.9334)\n",
            "Precision  = (0.9327)\n",
            "Recall  = (0.9341)\n",
            "Epoch Number =   8, Learning Rate = 0.0000698\n",
            "\tTraining Loop #  100, Loss = 0.1439\n",
            "\tTraining Loop #  200, Loss = 0.0819\n",
            "\tTraining Loop #  300, Loss = 0.2063\n",
            "\tTraining Loop #  400, Loss = 0.1292\n",
            "\tTraining Loop #  500, Loss = 0.1292\n",
            "\tTraining Loop #  600, Loss = 0.1333\n",
            "\tTraining Loop #  700, Loss = 0.2184\n",
            "\tTraining Loop #  800, Loss = 0.1959\n",
            "\tTraining Loop #  900, Loss = 0.2039\n",
            "\tTraining Loop # 1000, Loss = 0.2056\n",
            "\tTraining Loop # 1100, Loss = 0.0928\n",
            "\tTraining Loop # 1200, Loss = 0.2241\n",
            "\tTraining Loop # 1300, Loss = 0.1511\n",
            "\tTraining Loop # 1400, Loss = 0.0558\n",
            "\tTraining Loop # 1500, Loss = 0.1978\n",
            "\tValidation Loop #  10, Loss = 0.2330\n",
            "\tValidation Loop #  20, Loss = 0.1337\n",
            "\tValidation Loop #  30, Loss = 0.1523\n",
            "\tValidation Loop #  40, Loss = 0.1404\n",
            "\tValidation Loop #  50, Loss = 0.0993\n",
            "Epoch Number =  8, Time Taken (Mins) = 10.3264\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.1712)\n",
            "CrVal Loss  = (0.1693)\n",
            "Cr-Val Acc  = (0.9390)\n",
            "F1 Score  = (0.9327)\n",
            "Precision  = (0.9325)\n",
            "Recall  = (0.9328)\n",
            "Epoch Number =   9, Learning Rate = 0.0000663\n",
            "\tTraining Loop #  100, Loss = 0.1310\n",
            "\tTraining Loop #  200, Loss = 0.0798\n",
            "\tTraining Loop #  300, Loss = 0.1005\n",
            "\tTraining Loop #  400, Loss = 0.1078\n",
            "\tTraining Loop #  500, Loss = 0.0874\n",
            "\tTraining Loop #  600, Loss = 0.2514\n",
            "\tTraining Loop #  700, Loss = 0.2976\n",
            "\tTraining Loop #  800, Loss = 0.1165\n",
            "\tTraining Loop #  900, Loss = 0.2224\n",
            "\tTraining Loop # 1000, Loss = 0.0859\n",
            "\tTraining Loop # 1100, Loss = 0.1741\n",
            "\tTraining Loop # 1200, Loss = 0.1353\n",
            "\tTraining Loop # 1300, Loss = 0.1539\n",
            "\tTraining Loop # 1400, Loss = 0.1653\n",
            "\tTraining Loop # 1500, Loss = 0.1012\n",
            "\tValidation Loop #  10, Loss = 0.1740\n",
            "\tValidation Loop #  20, Loss = 0.1618\n",
            "\tValidation Loop #  30, Loss = 0.1861\n",
            "\tValidation Loop #  40, Loss = 0.1165\n",
            "\tValidation Loop #  50, Loss = 0.1373\n",
            "Epoch Number =  9, Time Taken (Mins) = 10.2240\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.1584)\n",
            "CrVal Loss  = (0.1627)\n",
            "Cr-Val Acc  = (0.9393)\n",
            "F1 Score  = (0.9321)\n",
            "Precision  = (0.9304)\n",
            "Recall  = (0.9338)\n",
            "Epoch Number =  10, Learning Rate = 0.0000630\n",
            "\tTraining Loop #  100, Loss = 0.1167\n",
            "\tTraining Loop #  200, Loss = 0.1341\n",
            "\tTraining Loop #  300, Loss = 0.1049\n",
            "\tTraining Loop #  400, Loss = 0.0923\n",
            "\tTraining Loop #  500, Loss = 0.0938\n",
            "\tTraining Loop #  600, Loss = 0.1975\n",
            "\tTraining Loop #  700, Loss = 0.2328\n",
            "\tTraining Loop #  800, Loss = 0.2514\n",
            "\tTraining Loop #  900, Loss = 0.2538\n",
            "\tTraining Loop # 1000, Loss = 0.1667\n",
            "\tTraining Loop # 1100, Loss = 0.1280\n",
            "\tTraining Loop # 1200, Loss = 0.1397\n",
            "\tTraining Loop # 1300, Loss = 0.0811\n",
            "\tTraining Loop # 1400, Loss = 0.2390\n",
            "\tTraining Loop # 1500, Loss = 0.1288\n",
            "\tValidation Loop #  10, Loss = 0.2502\n",
            "\tValidation Loop #  20, Loss = 0.3534\n",
            "\tValidation Loop #  30, Loss = 0.1287\n",
            "\tValidation Loop #  40, Loss = 0.1716\n",
            "\tValidation Loop #  50, Loss = 0.1798\n",
            "Epoch Number = 10, Time Taken (Mins) = 10.2537\n",
            "Model and Model Dictionary Saved\n",
            "Train Loss  = (0.1435)\n",
            "CrVal Loss  = (0.2398)\n",
            "Cr-Val Acc  = (0.9492)\n",
            "F1 Score  = (0.9415)\n",
            "Precision  = (0.9410)\n",
            "Recall  = (0.9421)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjRDONB2ZLlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d748496-7b78-4147-96e9-5de563f63ae9"
      },
      "source": [
        "  #Save Training Loss, Cross Validation Loss, Accuracy and F1 Score as images\n",
        "#Create X Axis for plotting\n",
        "xAxis = np.array(range(epoch + 1))\n",
        "print(xAxis)\n",
        "\n",
        "#Save Training Loss\n",
        "lossFigure(trainLossType, crvalLossType, xAxis, 'Train_Val_Loss_Type')\n",
        "#Save Accuracy\n",
        "saveImage(accuracyType, xAxis, 'Accuracy_Type')\n",
        "#Save F1 Score\n",
        "saveImage(f1ScoreType, xAxis, 'F1Score_Type')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsTPZQ-MZLlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graphs_save_path = \"/content/TRAINING_DATA/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EYFCgQlZLlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37a83479-dde1-4621-f0aa-4cf7c503b6da"
      },
      "source": [
        "shutil.copy('Train_Val_Loss_Type.png',graphs_save_path+'Train_Val_Loss_Type.png')\n",
        "shutil.copy('Accuracy_Type.png',graphs_save_path+'Accuracy_Type.png')\n",
        "shutil.copy('F1Score_Type.png',graphs_save_path+'F1Score_Type.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/TRAINING_DATA/F1Score_Type.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6htEAXTZLlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "9435b7d5-1599-4001-c0b1-9967a2c8d4cd"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>F1_Type</th>\n",
              "      <th>Recall_Type</th>\n",
              "      <th>Precision_Type</th>\n",
              "      <th>CrossvalAccuracy_Type</th>\n",
              "      <th>Train Loss_Type</th>\n",
              "      <th>CrVal Loss_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.807645</td>\n",
              "      <td>0.802970</td>\n",
              "      <td>0.812374</td>\n",
              "      <td>0.809607</td>\n",
              "      <td>0.752738</td>\n",
              "      <td>0.589769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.859689</td>\n",
              "      <td>0.857968</td>\n",
              "      <td>0.861418</td>\n",
              "      <td>0.865007</td>\n",
              "      <td>0.433449</td>\n",
              "      <td>0.397381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.892255</td>\n",
              "      <td>0.891662</td>\n",
              "      <td>0.892849</td>\n",
              "      <td>0.896914</td>\n",
              "      <td>0.337661</td>\n",
              "      <td>0.301440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.907860</td>\n",
              "      <td>0.909921</td>\n",
              "      <td>0.905809</td>\n",
              "      <td>0.915849</td>\n",
              "      <td>0.281761</td>\n",
              "      <td>0.237588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.916429</td>\n",
              "      <td>0.915508</td>\n",
              "      <td>0.917351</td>\n",
              "      <td>0.923913</td>\n",
              "      <td>0.240984</td>\n",
              "      <td>0.249514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.922892</td>\n",
              "      <td>0.921789</td>\n",
              "      <td>0.923998</td>\n",
              "      <td>0.929173</td>\n",
              "      <td>0.210690</td>\n",
              "      <td>0.199454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.933397</td>\n",
              "      <td>0.934095</td>\n",
              "      <td>0.932699</td>\n",
              "      <td>0.940743</td>\n",
              "      <td>0.189280</td>\n",
              "      <td>0.191904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.932678</td>\n",
              "      <td>0.932818</td>\n",
              "      <td>0.932537</td>\n",
              "      <td>0.938990</td>\n",
              "      <td>0.171207</td>\n",
              "      <td>0.169270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.932120</td>\n",
              "      <td>0.933840</td>\n",
              "      <td>0.930406</td>\n",
              "      <td>0.939341</td>\n",
              "      <td>0.158427</td>\n",
              "      <td>0.162722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.941539</td>\n",
              "      <td>0.942084</td>\n",
              "      <td>0.940995</td>\n",
              "      <td>0.949158</td>\n",
              "      <td>0.143464</td>\n",
              "      <td>0.239798</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Epoch   F1_Type  ...  Train Loss_Type  CrVal Loss_Type\n",
              "0    1.0  0.807645  ...         0.752738         0.589769\n",
              "1    2.0  0.859689  ...         0.433449         0.397381\n",
              "2    3.0  0.892255  ...         0.337661         0.301440\n",
              "3    4.0  0.907860  ...         0.281761         0.237588\n",
              "4    5.0  0.916429  ...         0.240984         0.249514\n",
              "5    6.0  0.922892  ...         0.210690         0.199454\n",
              "6    7.0  0.933397  ...         0.189280         0.191904\n",
              "7    8.0  0.932678  ...         0.171207         0.169270\n",
              "8    9.0  0.932120  ...         0.158427         0.162722\n",
              "9   10.0  0.941539  ...         0.143464         0.239798\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QngCWphb940z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f9f6e00-f603-487d-bf39-039f7a9bc080"
      },
      "source": [
        "df.to_excel('Training_data.xlsx')\n",
        "shutil.copy('Training_data.xlsx',graphs_save_path+'Training_data.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/TRAINING_DATA/Training_data.xlsx'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3V6qs1j-_zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import os\n",
        "from os.path import basename\n",
        "# create a ZipFile object\n",
        "\n",
        "dirName = '/content/TRAINING_DATA' \n",
        "\n",
        "with ZipFile('TRAINING_DATA2.zip', 'w') as zipObj:\n",
        "   # Iterate over all the files in directory\n",
        "   for folderName, subfolders, filenames in os.walk(dirName):\n",
        "       for filename in filenames:\n",
        "           #create complete filepath of file in directory\n",
        "           filePath = os.path.join(folderName, filename)\n",
        "           # Add file to zip\n",
        "           zipObj.write(filePath, basename(filePath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-27xq2D-tSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e72f1e5-3dce-4452-a224-1b4d064f2254"
      },
      "source": [
        "shutil.move('TRAINING_DATA2.zip','/content/drive/My Drive/RICE PROJECT/DATASET/Augumented Dataset/Classification_model/V1/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/RICE PROJECT/DATASET/Augumented Dataset/Classification_model/V1/TRAINING_DATA2.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKflmtZRZLmH",
        "colab_type": "text"
      },
      "source": [
        "Testing The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KtkeAUiZLmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "2e32ab90-d3d4-43b2-b940-02a976d76e00"
      },
      "source": [
        "# TEST CODE TO USE SAVED MODEL OR DICTIONARY\n",
        "\n",
        "testImageNames = list(testData['imageName'])\n",
        "testtarget1 = list(testData['value'])\n",
        "testRems = len(testImageNames) % batchSize\n",
        "testLoops = int((len(testImageNames) - testRems)/batchSize)\n",
        "\n",
        "#Steps to print by while testing\n",
        "testStepPrint = 2\n",
        "epoch = 0\n",
        "LearningRate  = 1.0e-4\n",
        "\n",
        "#Test the Models using the Test set\n",
        "testModels = ['8_0.9326776467381498.pth'] #ADD AS MANY MODELS AS YOU WANT TO TEST\n",
        "\n",
        "#Create Numpy arrays to sture metric of test models\n",
        "testLossType = np.zeros(len(testModels))\n",
        "\n",
        "accuracyType = np.zeros(len(testModels))\n",
        "\n",
        "f1ScoreType = np.zeros(len(testModels))\n",
        "\n",
        "#Initialize a model as a decoy to load the state dictionary\n",
        "model = riceClassifier()\n",
        "\n",
        "#Iterate over models to be tested\n",
        "for modelNum in range(len(testModels)):\n",
        "    #Print the name of the model being tested\n",
        "    modelLoadName = testModels[modelNum]\n",
        "    print(\"Model = {n:2s}\"\n",
        "          .format(n = modelLoadName))\n",
        "    \n",
        "    \n",
        "    #USE IF TESTING WITH MODEL\n",
        "    #Load the model to be tested\n",
        "    #model = torch.load(modelLoadName)\n",
        "    \n",
        "    #USE IF TESTING WITH DICTIONARY\n",
        "    model = riceClassifier()\n",
        "    model.load_state_dict(torch.load(r\"/content/TRAINING_DATA/MODELS/riceBreedModelDICTIONARY_8_0.9326776467381498.pth\"))\n",
        "    model.cuda()\n",
        "    model.float()\n",
        "    model.eval()\n",
        "    \n",
        "    #Initialize the optimizer and the loss criterion for the model\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr = LearningRate,\n",
        "                           amsgrad = True,\n",
        "                           )\n",
        "\n",
        "    #Initialize Confusion Matrix\n",
        "    confusionMatrixType = np.zeros((18, 18))\n",
        "    \n",
        "    #Initialize a variable to accumulate prediction time for every cutout\n",
        "    timePredict = 0\n",
        "    startTestTime = timer()\n",
        "    for i in range(testLoops + 1):\n",
        "        #Get Start index for batch\n",
        "        startNum = i * batchSize\n",
        "        \n",
        "        #Get Ending index for batch\n",
        "        if((startNum + batchSize - 1) < (len(testImageNames) - 1)):\n",
        "            endNum = startNum + batchSize - 1\n",
        "        else:\n",
        "            endNum = startNum + testRems - 1\n",
        "        \n",
        "        #input Tensors\n",
        "        \n",
        "        tensorList = list()\n",
        "        target1List = list()\n",
        "        \n",
        "        #Get List of Tensors from Images in dataset\n",
        "        for j in range(startNum, endNum + 1):\n",
        "            #Append transformed tensor of test images of user defined batch size\n",
        "            tensorList.append(makeTensorFromImageTrain(testImageNames[j]))\n",
        "            \n",
        "            #Append class of training output tensor to the class list\n",
        "            target1List.append(torch.FloatTensor([testtarget1[j]]))\n",
        "        \n",
        "        #Check if tensor list contains only one element\n",
        "        if(len(tensorList) < 2): break\n",
        "        \n",
        "        miniBatchInputTensor = torch.stack(tensorList)\n",
        "        miniBatchOutput1Tensor = torch.stack(target1List).squeeze()\n",
        "        \n",
        "        #GET CROSS VALIDATION LOSS\n",
        "        lossType, outputTensor1, = crossValidateModel(lossCriterion,\n",
        "                                         model,\n",
        "                                         miniBatchInputTensor,\n",
        "                                         miniBatchOutput1Tensor)\n",
        "        \n",
        "        #End the timer to calculate prediction time\n",
        "        endTestTime = timer()\n",
        "        \n",
        "        #Accumulate prediction time of mini-batch to 'timePredict' variable\n",
        "        timePredict += ((endTestTime - startTestTime)/batchSize)\n",
        "                \n",
        "        #Accumulate cross validation loss for rice type classification\n",
        "        testLossType[epoch] += lossType/(testLoops + 1)\n",
        "        #Update the epoch's confusion matrix for rice type classification\n",
        "        getConfusionMatrix_type(miniBatchOutput1Tensor, outputTensor1, confusionMatrixType)\n",
        "        if (((i + 1) % (epochStepPrint/10)) == 0) : \n",
        "            print(\"\\tValidation Loop #{a:4d}, Loss = {tl:6.4f}\"\n",
        "                  .format(a = (i + 1), tl = lossType))\n",
        "        \n",
        "    #TESTING ENDS HERE\n",
        "    print(\"Testing Done\")\n",
        "    #Print the Model\n",
        "    print(\"Model = {n:2s}, Prediction Time for one cutout = {pt:8.7f}\"\n",
        "          .format(n = modelLoadName, pt = (timePredict/(testLoops+1))))\n",
        "    #Find the values of required Metrics\n",
        "    precision1, recall1, f1ScoreType[modelNum], accuracyType[modelNum] = getF1Score_type(confusionMatrixType)\n",
        "   #Print test loss of rice type and grade classification\n",
        "    print(\"Test Loss  = ({l1:6.4f})\"\n",
        "              .format(l1 = crvalLossType[epoch]))\n",
        "        \n",
        "    #Print Test Accuracy of rice breed classification\n",
        "    print(\"Test Acc  = ({l1:6.4f})\"\n",
        "              .format(l1 = accuracyType[epoch]))\n",
        "        \n",
        "    #Print F1 Score of rice breed classification\n",
        "    print(\"F1 Score  = ({l1:6.4f})\"\n",
        "              .format(l1 = f1ScoreType[epoch]))\n",
        "        \n",
        "    #Print Precision of rice breed classification\n",
        "    print(\"Precision  = ({l1:6.4f})\"\n",
        "              .format(l1 = precision1))\n",
        "        \n",
        "    #Print Recall of rice breed classification\n",
        "    print(\"Recall  = ({l1:6.4f})\"\n",
        "              .format(l1 = recall1))\n",
        "    \n",
        "    np.set_printoptions(linewidth=100)\n",
        "    print(\"\\n\\nconfusion Matrix = \")\n",
        "    print(confusionMatrixType)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = 8_0.9326776467381498.pth\n",
            "\tValidation Loop #  10, Loss = 0.1059\n",
            "\tValidation Loop #  20, Loss = 0.1137\n",
            "\tValidation Loop #  30, Loss = 0.2309\n",
            "\tValidation Loop #  40, Loss = 0.1087\n",
            "\tValidation Loop #  50, Loss = 0.1427\n",
            "Testing Done\n",
            "Model = 8_0.9326776467381498.pth, Prediction Time for one cutout = 0.1506768\n",
            "Test Loss  = (0.5898)\n",
            "Test Acc  = (0.9530)\n",
            "F1 Score  = (0.9458)\n",
            "Precision  = (0.9453)\n",
            "Recall  = (0.9463)\n",
            "\n",
            "\n",
            "confusion Matrix = \n",
            "[[143.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   4.]\n",
            " [  0. 151.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
            " [  0.   0. 130.   0.   0.   0.   0.   7.   0.   0.   0.   0.   0.   6.   4.   0.   0.   0.]\n",
            " [  0.   0.   0. 183.   0.   0.   0.   0.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0. 134.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 108.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0. 143.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.  11.   0.   0.   0.   0. 114.   0.   0.   9.   0.   0.  12.   1.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0. 177.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0. 177.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.  18.   0.   0. 135.   0.   0.   5.   3.   0.   0.   2.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 162.   0.   0.   1.   0.   0.   0.]\n",
            " [  0.   0.   0.   3.   0.   0.   0.   0.   0.   6.   0.   0. 129.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   9.   0.   0.   0.   0.  18.   0.   0.   0.   0.   0. 130.   3.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1. 131.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 239.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 180.   0.]\n",
            " [  2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 150.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKfZ93XZLmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1370f2c0-5fa1-4d99-d62a-63f00e54fba4"
      },
      "source": [
        "from getpass import getpass\n",
        "password = getpass('Password')\n",
        "\n",
        "!git clone https://Pranesh6767:$password@github.com/Pranesh6767/Rice_Quality_Analysis.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password··········\n",
            "Cloning into 'Rice_Quality_Analysis'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 86836 (delta 4), reused 0 (delta 0), pack-reused 86824\u001b[K\n",
            "Receiving objects: 100% (86836/86836), 1.19 GiB | 30.20 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Checking out files: 100% (83427/83427), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BPC2yjncVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shutil.rmtree('/content/Rice_Quality_Analysis/Dataset/Cutouts')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYpdsnHZciQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e40688af-1e49-4786-c7e7-6d82f8ff456f"
      },
      "source": [
        "shutil.copy('/content/TRAINING_DATA/Accuracy_Type.png','/content/Rice_Quality_Analysis/Classification_model/V1/Graphs')\n",
        "shutil.copy('/content/TRAINING_DATA/F1Score_Type.png','/content/Rice_Quality_Analysis/Classification_model/V1/Graphs')\n",
        "shutil.copy('/content/TRAINING_DATA/Train_Val_Loss_Type.png','/content/Rice_Quality_Analysis/Classification_model/V1/Graphs')\n",
        "\n",
        "shutil.copy('/content/TRAINING_DATA/Training_data.xlsx','/content/Rice_Quality_Analysis/Classification_model/V1/Datasets')\n",
        "shutil.copy('/content/TRAINING_DATA/crValData.xlsx','/content/Rice_Quality_Analysis/Classification_model/V1/Datasets')\n",
        "shutil.copy('/content/TRAINING_DATA/testData.xlsx','/content/Rice_Quality_Analysis/Classification_model/V1/Datasets')\n",
        "shutil.copy('/content/TRAINING_DATA/testData.xlsx','/content/Rice_Quality_Analysis/Classification_model/V1/Datasets')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Rice_Quality_Analysis/Classification_model/V1/Datasets/testData.xlsx'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1buuaQafj_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = os.listdir('/content/TRAINING_DATA/MODELS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ-CEGe-fqqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d4e5e7ff-fbe0-4ae8-a40a-8287618f770f"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10_0.9415391261034298.pth',\n",
              " '3_0.8922550642887132.pth',\n",
              " 'riceBreedModelDICTIONARY_7_0.9333967564418123.pth',\n",
              " '7_0.9333967564418123.pth',\n",
              " '1_0.807644553527057.pth',\n",
              " 'riceBreedModelDICTIONARY_10_0.9415391261034298.pth',\n",
              " 'riceBreedModelDICTIONARY_1_0.807644553527057.pth',\n",
              " '5_0.9164288773593072.pth',\n",
              " 'riceBreedModelDICTIONARY_3_0.8922550642887132.pth',\n",
              " '4_0.9078603689904088.pth',\n",
              " '9_0.9321198612929722.pth',\n",
              " 'riceBreedModelDICTIONARY_4_0.9078603689904088.pth',\n",
              " 'riceBreedModelDICTIONARY_8_0.9326776467381498.pth',\n",
              " 'riceBreedModelDICTIONARY_2_0.8596894379367814.pth',\n",
              " '2_0.8596894379367814.pth',\n",
              " '6_0.9228920699969126.pth',\n",
              " 'riceBreedModelDICTIONARY_9_0.9321198612929722.pth',\n",
              " 'riceBreedModelDICTIONARY_6_0.9228920699969126.pth',\n",
              " '8_0.9326776467381498.pth',\n",
              " 'riceBreedModelDICTIONARY_5_0.9164288773593072.pth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzvMtNy_frJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in models:\n",
        "  shutil.copy('/content/TRAINING_DATA/MODELS/'+i,'/content/Rice_Quality_Analysis/Classification_model/V1/Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q-mT-Twf9UE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "00809d98-69c1-4ca7-a8cd-d8e5e0bb211a"
      },
      "source": [
        "%%bash\n",
        "cd /content/Rice_Quality_Analysis\n",
        "git config user.name \"Pranesh6767\"\n",
        "git config user.email \"pranesh.kulkarni18@vit.edu\"\n",
        "git add --all\n",
        "git commit -m 'Model 1 trained'  # commit in Colab\n",
        "git push origin master          # push to github"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date!\n",
            "Merge made by the 'recursive' strategy.\n",
            "On branch master\n",
            "Your branch is ahead of 'origin/master' by 2 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/Pranesh6767/Rice_Quality_Analysis\n",
            " * branch                master     -> FETCH_HEAD\n",
            "   92e7c1594..57c78c01d  master     -> origin/master\n",
            "To https://github.com/Pranesh6767/Rice_Quality_Analysis.git\n",
            "   57c78c01d..4d63be08c  master -> master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEpVbEPtghqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#git pull origin master\n",
        "#if required"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}